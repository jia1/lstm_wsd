Target to beat: 0.662

1.
With forward LSTM full length. No stemming. No stop words. No unsupervised training.
min_freq=5 => Vocabulary size: 14329
Cost (train/val): 2.053906/2.041744, Accuracy (train/val): 0.482063/0.471905


2.
As (1.), but with stemming.
min_freq=5 => Vocabulary size: 11079
Cost (train/val): 2.062860/2.011632, Accuracy (train/val): 0.479048/0.485238


3.
As (2.), but with variable input sequence length.
n_step = 20
EPOCH: 348
Cost (train/val): 1.589458/1.553583, Accuracy (train/val): 0.479677/0.484500

4.
As (3.), but with forward AND backward LSTM (bidirectional).
EPOCH: 138
Cost (train/val): 1.732096/1.760567, Accuracy (train/val): 0.482321/0.476250

5.
As 4, but fixed broadcasting bug.
EPOCH: 41
Cost (train/val): 0.791101/1.865055, Accuracy (train/val): 0.720645/0.480000

6.
As 5, but with glove.
EPOCH: 41
Cost (train/val): 0.727260/2.038966, Accuracy (train/val): 0.742097/0.456000

7.
As 6 + Dropout on embeddings.
EPOCH: 53
TRAIN ::: cost: 0.687805, accuracy: 0.754677
VAL ::: cost: 2.504641, accuracy: 0.392258

8.
As 7 + Gaussian noise on embeddings.

9.
As 8 + forget_bias = 0 in lstm.

10.
as 9 & not training embeddings.
::: EPOCH: 8 :::
TRAIN:: 	cost: 	1.290015, 	accuracy: 	0.559839
VAL:: 	cost: 	1.487941, 	accuracy: 	0.541000

11.
as 10 && BasicLSTM -> LSTM
::: EPOCH: 7 :::
TRAIN:: 	cost: 	1.349587, 	accuracy: 	0.547097
VAL:: 	cost: 	1.588297, 	accuracy: 	0.504000

12.
as 11 && use_peepholes=True
::: EPOCH: 8 :::
TRAIN --> 	cost: 	1.325734, 	accuracy: 	0.560000
VAL --> 	cost: 	1.569398, 	accuracy: 	0.496500

13.
2 * BasicLSTM, not training embedding. Only use state from L2.
not really improvement..

14.
1*BasicLSTM and dropout=0.7 4all. Training emb.
::: EPOCH: 6 :::
TRAIN -->       cost:   1.171895,       accuracy:       0.622742
VAL -->         cost:   1.516278,       accuracy:       0.543500

15.
as 15 && keep_prob = 0.2 4all.
::: EPOCH: 11 :::
TRAIN -->       cost:   1.329404,       accuracy:       0.584516
VAL -->         cost:   1.426725,       accuracy:       0.550000


todo
stop words
unsupervised
